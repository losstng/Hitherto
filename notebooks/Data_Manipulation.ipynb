{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ddacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e6991a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_12828\\2853018828.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  newsletter_df = pd.read_sql_query(\"SELECT * FROM newsletter\", conn)\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(   \n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Fred3001!\",\n",
    "    port=\"5432\")\n",
    "\n",
    "newsletter_df = pd.read_sql_query(\"SELECT * FROM newsletter\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c78e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsletter_df = newsletter_df.sort_values(by='received_at', ascending=False)\n",
    "newsletter_df = newsletter_df.drop(columns=['id','sender','extracted_text', 'message_id', 'token_count', 'vectorized', 'created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaae4d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>received_at</th>\n",
       "      <th>chunked_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>UBS's profit beats</td>\n",
       "      <td>morning_briefing:_europe</td>\n",
       "      <td>2025-07-30 07:33:39</td>\n",
       "      <td>[Morning Briefing: Europe \\n Bloomberg Morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Breaking News: Japan Issues Tsunami Warning fo...</td>\n",
       "      <td>bloomberg_news_alert</td>\n",
       "      <td>2025-07-30 03:15:53</td>\n",
       "      <td>[Bloomberg News Alert \\n\\n\\n \\n\\n Japan Issues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Money Stuff: UBS FX Trades Were Too Good</td>\n",
       "      <td>money_stuff</td>\n",
       "      <td>2025-07-29 20:13:50</td>\n",
       "      <td>[Money Stuff \\n FX, Sinovac, Daily Journal, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Industrial action</td>\n",
       "      <td>bloomberg_deals</td>\n",
       "      <td>2025-07-29 19:44:32</td>\n",
       "      <td>[Bloomberg Deals \\n Plus: Iveco nears deals wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Harsh reality</td>\n",
       "      <td>evening_briefing:_europe</td>\n",
       "      <td>2025-07-29 18:37:08</td>\n",
       "      <td>[Evening Briefing: Europe \\n Evening Briefing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Economics Daily: ‘Stockholm syndrome’</td>\n",
       "      <td>economics_daily</td>\n",
       "      <td>2025-07-29 13:06:38</td>\n",
       "      <td>[Economics Daily \\n I’m Chris Anstey, an econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Sony's anime music goes platinum</td>\n",
       "      <td>tech_in_depth</td>\n",
       "      <td>2025-07-29 13:04:47</td>\n",
       "      <td>[Tech In Depth \\n Welcome to Tech In Depth, ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Supply Lines: Now for the fine print</td>\n",
       "      <td>supply_lines</td>\n",
       "      <td>2025-07-29 13:00:37</td>\n",
       "      <td>[Supply Lines \\n It took less than 48 hours af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Reiches Sommertour mit Tücken: Fünf Themen des...</td>\n",
       "      <td>five_things:_germany</td>\n",
       "      <td>2025-07-29 12:41:27</td>\n",
       "      <td>[Five Things: Germany \\n Nick Heubeck über get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Preventing lip cancer</td>\n",
       "      <td>prognosis</td>\n",
       "      <td>2025-07-29 12:00:48</td>\n",
       "      <td>[Prognosis \\n Where your skin is so thin. \\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "183                                 UBS's profit beats   \n",
       "311  Breaking News: Japan Issues Tsunami Warning fo...   \n",
       "192           Money Stuff: UBS FX Trades Were Too Good   \n",
       "193                                  Industrial action   \n",
       "194                                      Harsh reality   \n",
       "131              Economics Daily: ‘Stockholm syndrome’   \n",
       "134                   Sony's anime music goes platinum   \n",
       "77                Supply Lines: Now for the fine print   \n",
       "78   Reiches Sommertour mit Tücken: Fünf Themen des...   \n",
       "79                               Preventing lip cancer   \n",
       "\n",
       "                     category         received_at  \\\n",
       "183  morning_briefing:_europe 2025-07-30 07:33:39   \n",
       "311      bloomberg_news_alert 2025-07-30 03:15:53   \n",
       "192               money_stuff 2025-07-29 20:13:50   \n",
       "193           bloomberg_deals 2025-07-29 19:44:32   \n",
       "194  evening_briefing:_europe 2025-07-29 18:37:08   \n",
       "131           economics_daily 2025-07-29 13:06:38   \n",
       "134             tech_in_depth 2025-07-29 13:04:47   \n",
       "77               supply_lines 2025-07-29 13:00:37   \n",
       "78       five_things:_germany 2025-07-29 12:41:27   \n",
       "79                  prognosis 2025-07-29 12:00:48   \n",
       "\n",
       "                                          chunked_text  \n",
       "183  [Morning Briefing: Europe \\n Bloomberg Morning...  \n",
       "311  [Bloomberg News Alert \\n\\n\\n \\n\\n Japan Issues...  \n",
       "192  [Money Stuff \\n FX, Sinovac, Daily Journal, in...  \n",
       "193  [Bloomberg Deals \\n Plus: Iveco nears deals wi...  \n",
       "194  [Evening Briefing: Europe \\n Evening Briefing ...  \n",
       "131  [Economics Daily \\n I’m Chris Anstey, an econo...  \n",
       "134  [Tech In Depth \\n Welcome to Tech In Depth, ou...  \n",
       "77   [Supply Lines \\n It took less than 48 hours af...  \n",
       "78   [Five Things: Germany \\n Nick Heubeck über get...  \n",
       "79   [Prognosis \\n Where your skin is so thin. \\n\\n...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsletter_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92842e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA from 2025-08-01 to 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$NVDA: possibly delisted; no price data found  (1m 2025-08-01 -> 2025-08-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data returned for NVDA.\n",
      "Fetching INOD from 2025-08-01 to 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$INOD: possibly delisted; no price data found  (1m 2025-08-01 -> 2025-08-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data returned for INOD.\n",
      "Fetching MRVL from 2025-08-01 to 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$MRVL: possibly delisted; no price data found  (1m 2025-08-01 -> 2025-08-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data returned for MRVL.\n",
      "Fetching TSLA from 2025-08-01 to 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$TSLA: possibly delisted; no price data found  (1m 2025-08-01 -> 2025-08-09)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data returned for TSLA.\n",
      "Fetching NVDA daily data from 2025-08-01 to 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\Scripts\\data.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_df = pd.concat([old_df, df], sort=False)\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\Scripts\\data.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_df = pd.concat([old_df, df], sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for NVDA saved to daily_NVDA.csv.\n",
      "Fetching INOD daily data from 2025-08-01 to 2025-08-09...\n",
      "Data for INOD saved to daily_INOD.csv.\n",
      "Fetching MRVL daily data from 2025-08-01 to 2025-08-09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\Scripts\\data.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_df = pd.concat([old_df, df], sort=False)\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\Scripts\\data.py:100: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  merged_df = pd.concat([old_df, df], sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for MRVL saved to daily_MRVL.csv.\n",
      "Fetching TSLA daily data from 2025-08-01 to 2025-08-09...\n",
      "Data for TSLA saved to daily_TSLA.csv.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "REPO_ROOT = Path.cwd().parents[0]  # going back once from the current directory\n",
    "SCRIPT_PATH = REPO_ROOT / \"Scripts\"\n",
    "sys.path.append(str(SCRIPT_PATH))\n",
    "from data import update_intraday_csv, update_daily_csv\n",
    "\n",
    "update_intraday_csv(\"NVDA\")\n",
    "update_intraday_csv(\"INOD\")\n",
    "update_intraday_csv(\"MRVL\")\n",
    "update_intraday_csv(\"TSLA\")\n",
    "\n",
    "update_daily_csv(\"NVDA\")\n",
    "update_daily_csv(\"INOD\")\n",
    "update_daily_csv(\"MRVL\")\n",
    "update_daily_csv(\"TSLA\")\n",
    "\n",
    "intraday_nvd = pd.read_csv('..\\\\raw_data\\\\intraday\\\\intraday_NVDA.csv')\n",
    "intraday_mrvl = pd.read_csv('..\\\\raw_data\\\\intraday\\\\intraday_MRVL.csv')\n",
    "intraday_inod = pd.read_csv('..\\\\raw_data\\\\intraday\\\\intraday_INOD.csv')\n",
    "intraday_tsla = pd.read_csv('..\\\\raw_data\\\\intraday\\\\intraday_TSLA.csv')\n",
    "\n",
    "daily_nvd = pd.read_csv('..\\\\raw_data\\\\daily_NVDA.csv')\n",
    "daily_mrvl = pd.read_csv('..\\\\raw_data\\\\daily_MRVL.csv')\n",
    "daily_inod = pd.read_csv('..\\\\raw_data\\\\daily_INOD.csv')\n",
    "daily_tsla = pd.read_csv('..\\\\raw_data\\\\daily_TSLA.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704302a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date       Open       High        Low      Close  \\\n",
      "0  2025-07-01 00:00:00-04:00  50.099998  50.860001  46.299999  47.389999   \n",
      "1  2025-07-02 00:00:00-04:00  46.509998  48.549999  46.150002  47.939999   \n",
      "2  2025-07-03 00:00:00-04:00  48.500000  50.709999  48.470001  50.130001   \n",
      "3  2025-07-07 00:00:00-04:00  48.889999  51.340000  47.810001  50.990002   \n",
      "4  2025-07-08 00:00:00-04:00  51.730000  55.689999  49.750000  50.970001   \n",
      "5  2025-07-09 00:00:00-04:00  51.689999  52.580002  49.410000  52.040001   \n",
      "6  2025-07-10 00:00:00-04:00  53.130001  53.860001  50.470001  52.700001   \n",
      "7  2025-07-11 00:00:00-04:00  52.000000  52.660000  49.290001  49.720001   \n",
      "8  2025-07-14 00:00:00-04:00  49.430000  50.580002  47.320000  50.340000   \n",
      "9  2025-07-15 00:00:00-04:00  50.880001  51.500000  48.369999  49.099998   \n",
      "\n",
      "    Volume  Dividends  Stock Splits  \n",
      "0  2774900        0.0           0.0  \n",
      "1  2516200        0.0           0.0  \n",
      "2  1526300        0.0           0.0  \n",
      "3  1733600        0.0           0.0  \n",
      "4  2922400        0.0           0.0  \n",
      "5  1640700        0.0           0.0  \n",
      "6  1633900        0.0           0.0  \n",
      "7  1630100        0.0           0.0  \n",
      "8  1754500        0.0           0.0  \n",
      "9  1651800        0.0           0.0  \n",
      "                    Datetime       Open       High        Low      Close  \\\n",
      "0  2025-06-23 09:30:00-04:00  46.915001  47.129700  46.630001  46.959999   \n",
      "1  2025-06-23 09:31:00-04:00  46.799999  46.799999  46.119701  46.330002   \n",
      "2  2025-06-23 09:32:00-04:00  46.200001  46.200001  44.520500  44.660000   \n",
      "3  2025-06-23 09:33:00-04:00  44.660000  44.930698  44.340000  44.801201   \n",
      "4  2025-06-23 09:34:00-04:00  44.500000  44.918201  44.340000  44.860001   \n",
      "5  2025-06-23 09:35:00-04:00  44.806599  44.915001  44.000000  44.074299   \n",
      "6  2025-06-23 09:36:00-04:00  44.040001  44.641399  44.000000  44.520000   \n",
      "7  2025-06-23 09:37:00-04:00  44.580002  44.900002  44.435001  44.830002   \n",
      "8  2025-06-23 09:38:00-04:00  44.910000  44.977600  44.740002  44.904999   \n",
      "9  2025-06-23 09:39:00-04:00  44.929401  45.310001  44.810001  45.279999   \n",
      "\n",
      "   Volume  Dividends  Stock Splits  \n",
      "0  335388        0.0           0.0  \n",
      "1   58983        0.0           0.0  \n",
      "2   60705        0.0           0.0  \n",
      "3  233020        0.0           0.0  \n",
      "4   71585        0.0           0.0  \n",
      "5   86815        0.0           0.0  \n",
      "6   36101        0.0           0.0  \n",
      "7   34662        0.0           0.0  \n",
      "8   58491        0.0           0.0  \n",
      "9   27922        0.0           0.0  \n",
      "                                                 title  \\\n",
      "183                                 UBS's profit beats   \n",
      "311  Breaking News: Japan Issues Tsunami Warning fo...   \n",
      "192           Money Stuff: UBS FX Trades Were Too Good   \n",
      "193                                  Industrial action   \n",
      "194                                      Harsh reality   \n",
      "131              Economics Daily: ‘Stockholm syndrome’   \n",
      "134                   Sony's anime music goes platinum   \n",
      "77                Supply Lines: Now for the fine print   \n",
      "78   Reiches Sommertour mit Tücken: Fünf Themen des...   \n",
      "79                               Preventing lip cancer   \n",
      "\n",
      "                     category         received_at  \\\n",
      "183  morning_briefing:_europe 2025-07-30 07:33:39   \n",
      "311      bloomberg_news_alert 2025-07-30 03:15:53   \n",
      "192               money_stuff 2025-07-29 20:13:50   \n",
      "193           bloomberg_deals 2025-07-29 19:44:32   \n",
      "194  evening_briefing:_europe 2025-07-29 18:37:08   \n",
      "131           economics_daily 2025-07-29 13:06:38   \n",
      "134             tech_in_depth 2025-07-29 13:04:47   \n",
      "77               supply_lines 2025-07-29 13:00:37   \n",
      "78       five_things:_germany 2025-07-29 12:41:27   \n",
      "79                  prognosis 2025-07-29 12:00:48   \n",
      "\n",
      "                                          chunked_text  \n",
      "183  [Morning Briefing: Europe \\n Bloomberg Morning...  \n",
      "311  [Bloomberg News Alert \\n\\n\\n \\n\\n Japan Issues...  \n",
      "192  [Money Stuff \\n FX, Sinovac, Daily Journal, in...  \n",
      "193  [Bloomberg Deals \\n Plus: Iveco nears deals wi...  \n",
      "194  [Evening Briefing: Europe \\n Evening Briefing ...  \n",
      "131  [Economics Daily \\n I’m Chris Anstey, an econo...  \n",
      "134  [Tech In Depth \\n Welcome to Tech In Depth, ou...  \n",
      "77   [Supply Lines \\n It took less than 48 hours af...  \n",
      "78   [Five Things: Germany \\n Nick Heubeck über get...  \n",
      "79   [Prognosis \\n Where your skin is so thin. \\n\\n...  \n"
     ]
    }
   ],
   "source": [
    "print(daily_inod.head(10))\n",
    "print(intraday_inod.head(10))\n",
    "print(newsletter_df.head(10))\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "\n",
    "def clean_daily_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the daily data DataFrame by removing unnecessary columns and\n",
    "    converting timezone from US/Eastern to Europe/Berlin.\n",
    "    \"\"\"\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    if df['Date'].dt.tz is None:\n",
    "        df['Date'] = df['Date'].dt.tz_localize('US/Eastern')\n",
    "    df['Date'] = df['Date'].dt.tz_convert('Europe/Berlin')\n",
    "    df = df.drop(columns=['Volume', 'Dividends', 'Stock Splits'])\n",
    "    return df\n",
    "\n",
    "def clean_intraday_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the intraday data DataFrame by removing unnecessary columns, renaming others,\n",
    "    and converting timezone from US/Eastern to Europe/Berlin.\n",
    "    \"\"\"\n",
    "    # Ensure datetime is parsed\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    # If tz-naive, localize to US/Eastern; then convert to Europe/Berlin\n",
    "    if df['Datetime'].dt.tz is None:\n",
    "        df['Datetime'] = df['Datetime'].dt.tz_localize('US/Eastern')\n",
    "    df['Datetime'] = df['Datetime'].dt.tz_convert('Europe/Berlin')\n",
    "\n",
    "    # Extract components **after** timezone conversion\n",
    "    df['Date'] = df['Datetime'].dt.date\n",
    "    df['Time'] = df['Datetime'].dt.time\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    df['Minute'] = df['Datetime'].dt.minute\n",
    "\n",
    "    # Drop unneeded columns if they exist\n",
    "    for col in ['Dividends', 'Stock Splits']:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=col)\n",
    "    df = df.drop(columns=['Datetime'])\n",
    "    return df\n",
    "\n",
    "intraday_inod = clean_intraday_data(intraday_inod)\n",
    "intraday_mrvl = clean_intraday_data(intraday_mrvl)\n",
    "intraday_nvd = clean_intraday_data(intraday_nvd)\n",
    "intraday_tsla = clean_intraday_data(intraday_tsla)\n",
    "\n",
    "daily_inod = clean_daily_data(daily_inod)\n",
    "daily_mrvl = clean_daily_data(daily_mrvl)\n",
    "daily_nvd = clean_daily_data(daily_nvd)\n",
    "daily_tsla = clean_daily_data(daily_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66fe9750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Date       Open       High        Low      Close\n",
      "0 2025-07-01 06:00:00+02:00  50.099998  50.860001  46.299999  47.389999\n",
      "        Open     High        Low      Close  Volume        Date      Time  \\\n",
      "0  46.915001  47.1297  46.630001  46.959999  335388  2025-06-23  15:30:00   \n",
      "\n",
      "   Hour  Minute  \n",
      "0    15      30  \n",
      "                  title                  category         received_at  \\\n",
      "183  UBS's profit beats  morning_briefing:_europe 2025-07-30 07:33:39   \n",
      "\n",
      "                                          chunked_text  \n",
      "183  [Morning Briefing: Europe \\n Bloomberg Morning...  \n"
     ]
    }
   ],
   "source": [
    "print(daily_inod.head(1))\n",
    "print(intraday_inod.head(1))\n",
    "print(newsletter_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e531a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_returns_to_entries(entries_df, daily_ohlc_df,\n",
    "                           entry_date_col='received_at',\n",
    "                           price_date_col='Date',\n",
    "                           return_days=[1, 3]):\n",
    "    \"\"\"\n",
    "    Adds future returns to an entries dataframe based on close prices in daily OHLC data.\n",
    "    Adjusts for weekends when calculating returns.\n",
    "    \"\"\"\n",
    "    entries_df = entries_df.copy()\n",
    "    daily_ohlc_df = daily_ohlc_df.copy()\n",
    "\n",
    "    # Normalize to timezone-naive date\n",
    "    entries_df['entry_date'] = pd.to_datetime(entries_df[entry_date_col], errors='coerce').dt.date\n",
    "    daily_ohlc_df['price_date'] = pd.to_datetime(daily_ohlc_df[price_date_col], errors='coerce').dt.tz_localize(None).dt.date\n",
    "\n",
    "    # Prepare lookup\n",
    "    trading_days = sorted(daily_ohlc_df['price_date'].dropna().unique())\n",
    "    close_price_map = daily_ohlc_df.set_index('price_date')['Close'].to_dict()\n",
    "\n",
    "    # Efficient trading day lookup with upper bound\n",
    "    def get_t_plus_n(entry_date, n):\n",
    "        if pd.isnull(entry_date):\n",
    "            return None\n",
    "        try:\n",
    "            idx = trading_days.index(entry_date)\n",
    "            if idx + n < len(trading_days):\n",
    "                return trading_days[idx + n]\n",
    "            else:\n",
    "                return None  # No future price available\n",
    "        except ValueError:\n",
    "            return None  # entry_date not in trading_days\n",
    "\n",
    "    for n in return_days:\n",
    "        t_col = f't_plus_{n}'\n",
    "        ret_col = f'{n}d_return'\n",
    "\n",
    "        # Safely map T+n date\n",
    "        entries_df[t_col] = entries_df['entry_date'].apply(lambda x: get_t_plus_n(x, n))\n",
    "\n",
    "        # Compute return\n",
    "        def calc_return(row):\n",
    "            try:\n",
    "                p0 = close_price_map.get(row['entry_date'])\n",
    "                pn = close_price_map.get(row[t_col])\n",
    "                if p0 is None or pn is None:\n",
    "                    return None\n",
    "                return (pn - p0) / p0\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        entries_df[ret_col] = entries_df.apply(calc_return, axis=1)\n",
    "\n",
    "    return entries_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b060d317",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfBoundsDatetime",
     "evalue": "Out of bounds nanosecond timestamp: 9223372800000000000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOverflowError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/timestamps.pyx:451\u001b[39m, in \u001b[36mpandas._libs.tslibs.timestamps._Timestamp.__add__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOverflowError\u001b[39m: value too large",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutOfBoundsDatetime\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m newsletter_df = \u001b[43madd_returns_to_entries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewsletter_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaily_inod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mentry_date_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreceived_at\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mprice_date_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mreturn_days\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(newsletter_df.head(\u001b[32m10\u001b[39m).sort_values(by=\u001b[33m'\u001b[39m\u001b[33mreceived_at\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36madd_returns_to_entries\u001b[39m\u001b[34m(entries_df, daily_ohlc_df, entry_date_col, price_date_col, return_days)\u001b[39m\n\u001b[32m     45\u001b[39m t_col = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt_plus_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     46\u001b[39m ret_col = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33md_return\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m entries_df[t_col] = \u001b[43mentries_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentry_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_trading_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnotnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_return\u001b[39m(row):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\Hitherto\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36madd_returns_to_entries.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     45\u001b[39m t_col = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt_plus_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     46\u001b[39m ret_col = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33md_return\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     48\u001b[39m entries_df[t_col] = entries_df[\u001b[33m'\u001b[39m\u001b[33mentry_date\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mnext_trading_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m pd.notnull(x) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     50\u001b[39m )\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_return\u001b[39m(row):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36madd_returns_to_entries.<locals>.next_trading_day\u001b[39m\u001b[34m(date)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43mdate\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m date.date() \u001b[38;5;129;01min\u001b[39;00m trading_days:\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m date.date()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/timestamps.pyx:458\u001b[39m, in \u001b[36mpandas._libs.tslibs.timestamps._Timestamp.__add__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOutOfBoundsDatetime\u001b[39m: Out of bounds nanosecond timestamp: 9223372800000000000"
     ]
    }
   ],
   "source": [
    "newsletter_df = add_returns_to_entries(newsletter_df, daily_inod,entry_date_col='received_at', \n",
    "                           price_date_col='Date', \n",
    "                           return_days=[1, 3])\n",
    "\n",
    "print(newsletter_df.head(10).sort_values(by='received_at', ascending=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
